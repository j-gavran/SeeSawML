Training is handled by PyTorch Lightning's [`Trainer`](https://lightning.ai/docs/pytorch/stable/common/trainer.html) class, which provides a high-level interface for training models. The training configuration options are specified in the `experiment_config` field of the `training_config.yaml` files. Training is performed by running `train_signal` or `train_fakes` commands.

## Global Options

### Experiment Settings

- `experiment_name: str`: Name of the training experiment. Used for logging and checkpointing.
- `run_name: str | None`: Optional name for the specific training run. If not provided, a default name based on timestamp will be used.
- `save_dir: str | None`: Directory where to save model checkpoints and logs. If set to `null`, defaults to `ANALYSIS_ML_LOGS_DIR`.
- `tracker_path: str | None`: Path for the experiment tracker (i.e., plotting when training). If set to `null`, defaults to `ANALYSIS_ML_RESULTS_DIR`.
- `seed: int | None`: Random seed for reproducibility. If set to `null`, no seed is set.

### Trainer Settings

- `accelerator: str`: Accelerator to use for training. Options are `cpu`, `gpu`, or `auto`.
- `devices: int | list[int] | str`: Number of devices to train on (`int`), list of devices (`list[int]` or `str`), or `auto`.
- `precision: int | str`: Double precision (`64` or `64-true`), full precision (`32` or `32-true`), 16bit mixed precision (`16` or `16-mixed`) or bfloat16 mixed precision (`bf16` or `bf16-mixed`).
- `float32_matmul_precision: str`: Precision for float32 matrix multiplications. Options are `highest`, `high`, or `medium`. See [PyTorch docs](https://docs.pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html) for more details.
- `check_eval_n_epoch: int`: Frequency (in epochs) to run evaluation on the validation set, by default `1`.
- `num_sanity_val_steps: int`: Number of validation steps to run before training to catch bugs, by default `0`.

### Logging and Metric Tracking Settings

- `comet_api_key: str | None`: Comet API key for experiment tracking. If set to `null`, Comet tracking is disabled.
- `comet_project_name: str | None`: Comet project name for experiment tracking. If set to `null`, uses default project.
- `log_every_n_steps: int`: Frequency (in steps) to log training metrics.
- `check_metrics_n_epoch: int`: Frequency (in epochs) to compute tracker metrics.
- `plot_metrics_n_epoch: int`: Frequency (in epochs) to plot tracker metrics.
- `tqdm_refresh_rate: int`: Refresh rate for the progress bar.

!!! Tip "MLflow Experiment Tracking"
    MLFlow can be launched locally by running `track -p <port_number>` in a terminal and then accessing the UI at `http://localhost:<port_number>` in a web browser. All training runs will be logged automatically to the `mlruns/` directory inside `ANALYSIS_ML_LOGS_DIR`.

!!! Note "Using Comet for Experiment Tracking"
    If running locally and not on a cluster, it is recommended to use MLFlow for experiment tracking instead of Comet. This can be set by omitting the `comet_api_key` and `comet_project_name` fields in the configuration or setting them to `null`.

!!! Info "Plotting On the Fly During Validation"
    SeeSawML supports plotting of model performance and custom metrics on validation set during training. This is handled by the internal [`Tracker`](https://seesawml.docs.cern.ch/api/models/training/#seesawml.models.tracker.Tracker) class. If using MLFlow the plots can be viewed in the MLFlow UI under the "Artifacts" section. Note that plotting during training may slow down the training process and leak memory if too many plots are generated by matplotlib. It is recommended to set `plot_metrics_n_epoch` to reasonable values (e.g., every 5 or 10 epochs or even disable it by setting it to `null`) to avoid excessive plotting.

## Model Training Configuration

The configuration for each model type can be found in the `model_config/` directory. There are two top level YAML fields that are common to all models: `architecture_config` and `training_config`. The `training_config` field contains options related to the training process and is common across all models and is described below.

Some examples of model training configurations can be found [here](../examples/model_configs.md).

### Common Configuration

Common configuration options that apply to all models are specified at the top level of the model configuration file.

- `name: str`: Name of the model.
- `load_checkpoint: str | None`: Name of the checkpoint file to load the model weights from, by default `null`. If `null`, the model will be trained from scratch. Path to this checkpoint is configured in the `training_config.model_save_path`.
- `continue_training: bool`: Whether to continue training from the loaded checkpoint, by default `false`. If set to `true`, needs to be used in conjunction with `load_checkpoint`.

### Loss

- `loss: dict[str | Any] | str`: Specifies the loss function to be used during training. If a string is provided, it is interpreted as the name of the loss function. If a dictionary is provided, it should contain the following fields:
    - `loss_name: str`: The name of the loss function if `loss` is provided as a dictionary.
    - `loss_params: dict[str | Any]`: Parameters specific to the chosen loss function if `loss` is provided as a dictionary. Some loss functions may not require any parameters.

- `disco: dict[str | Any] | None`: Distance correlation regularization to reduce mass sculpting from [https://arxiv.org/abs/2001.05310](https://arxiv.org/abs/2001.05310), by default `null`.
    - `variables: list[str]`: List of variable names to apply distance correlation regularization on.
    - `lambda: float`: Regularization strength.
    - `power: float`: Power to which the distance correlation is raised, by default `1.0`.
    - `weighted: bool`: Whether to use class or MC weights in the distance correlation calculation, by default `false`.
    - `multiclass_reduction: str`: Reduction method for multiclass distance correlation (`logits` or `entropy`), by default `logits`.

!!! Info "Supported Loss Functions"

    Supported loss functions for signal vs background classification include:

    - `BCEWithLogitsLoss`: Binary classification loss with logits (**good baseline**).
    - `MSELoss`: Mean Squared Error loss for regression tasks.
    - `CrossEntropyLoss`: Standard cross-entropy loss for multi-class classification (**good baseline**).
    - `SigmoidFocalLoss`: Focal loss for addressing class imbalance in binary classification.
    - `MulticlassFocalLoss`: Focal loss for multi-class classification tasks.
    - All the losses from `pytorch_optimizer` package listed [here](https://pytorch-optimizers.readthedocs.io/en/latest/loss/).

    Supported loss functions for fakes estimation include different options, see [here](https://gitlab.cern.ch/atlas-dch-seesaw-analyses/SeeSawML/-/blob/main/seesaw/fakes/models/loss.py?ref_type=heads#L3).

### Optimizer

- `optimizer: dict[str | Any]`: Specifies the optimizer to be used during training.
    - `optimizer_name: str`: The name of the optimizer.
    - `optimizer_params: dict[str | Any]`: Parameters specific to the chosen optimizer.

!!! Info "Supported Optimizers"
    Supported optimizers include all optimizers from PyTorch listed [here](https://docs.pytorch.org/docs/stable/optim.html#algorithms) as well as those from `pytorch_optimizer` package listed [here](https://pytorch-optimizers.readthedocs.io/en/latest/optimizer/). A good default choice is `Adam` or `AdamW` with a learning rate of `3e-4`.

### Scheduler

- `scheduler: dict[str | Any] | None`: Learning rate scheduler to adjust the learning rate during training.
    - `scheduler_name: str`: The name of the scheduler.
    - `interval: str`: Interval for scheduler step (`epoch` or `step`).
    - `scheduler_params: dict[str | Any]`: Parameters specific to the chosen scheduler.
- `reduce_lr_on_epoch: float | None`: If set, reduces the learning rate by a factor every epoch. Can be used as an alternative to schedulers, by default `null`.

!!! Info "Supported Schedulers"
    Supported schedulers include all schedulers from PyTorch listed [here](https://docs.pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) as well as those from `pytorch_optimizer` package listed [here](https://pytorch-optimizers.readthedocs.io/en/latest/lr_scheduler/). Additionally, the following custom schedulers are available:

    - `AttentionWarmup`: Linear warmup followed by a decay.
    - `SqrtExpWarmup`: Combines square root decay with exponential warmup.
    - `CosineWarmup`: Cosine annealing with linear warmup.
    - `LinearWarmup`: Linear warmup followed by a constant learning rate.

### Other Training Options

- `max_epochs: int`: Maximum number of training epochs.
- `early_stop_patience: int | None`: Number of epochs with no improvement after which training will be stopped. If `null`, early stopping is disabled.
- `monitor: str`: Metric to monitor for early stopping and learning rate scheduling, by default `val_loss`.
- `monitor_mode: str`: Mode for monitoring the metric (`min` or `max`), by default `min`.
- `save_top_k: int`: Number of best models to save based on the monitored metric, by default `1`.
- `gradient_clip_val: float | None`: Maximum norm for gradient clipping, by default `null` (disabled).
- `log_train_memory: bool`: Whether to log memory usage during training, by default `false`. Used for debugging memory issues.
- `model_save_path: str | None`: Path where to save the model checkpoints. If `null`, defaults to `ANALYSIS_ML_MODELS_DIR/checkpoints`.

!!! Info "Fakes Model Regularization Options"
    Fakes models additionally support the following training configuration options:

    - `w_lambda: float | None`: Regularization strength for density estimation, by default `null`.
    - `ess_lambda: float | None`: Regularization strength for effective sample size, by default `null`.

!!! Example "Simple Training Configuration"
    A simple binary classifier training configuration might look like this:

    ```yaml
    name: sigBkgClassifier

    load_checkpoint: null
    continue_training: false

    architecture_config:
      ...

    training_config:
      loss: bce

      optimizer:
        optimizer_name: Adam
        optimizer_params:
          lr: 1.0e-3
          weight_decay: 1.0e-5

      scheduler:
        scheduler_name: ReduceLROnPlateau
        interval: epoch
        scheduler_params:
          factor: 0.5
          patience: 5

      max_epochs: 100
      early_stop_patience: 10
      monitor: val_loss
      save_top_k: 5
      model_save_path: null
    ```

    Note that this is part of the model configuration YAML file located in the `model_config/` directory.

    After the model has been trained, it can loaded using the `load_checkpoint` (name of the `.ckpt` file) and `model_save_path` (path to the checkpoint directory) fields in the same configuration file for evaluation or further training.
